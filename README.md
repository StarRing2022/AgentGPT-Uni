# ChatGLM-Uni
# Can lora weights be used Exchange-Fusion in different large language models?
# lora能在不同的大语言模型间交叉融合使用吗？

## 介绍（Introduce）

## 方法（Method）

## 实验环境(Trial Environment)

## 实验结果(Trial Result)

## 结论与后续课题(Conclusion and Future Work)


## 更新日志(Update Log)
2023-04-09<br>
The initial start of the project can use the weights trained by LLAMA Alpaca for ChatGlm. Compared to LLAMA, ChatGlm has a smaller model size and deployment cost, which is close to the effect of LLAMA, especially in terms of English performance.<br>
工程初步启动，可使用LLAMA Alpaca训练的权值，用于ChatGlm，相比LLAMA，ChatGlm以更小的模型体积和部署成本，接近于LLAMA的效果，尤其在英文的表现上.<br><br>

## 共创共赢(Join This Work)
If you are interested in our work, please give "Fork" or "Star" your attention and support. We will be grateful much, or contact us. QR code:<br>
如您对我们的工作产生兴趣，请给予小星星Star关注和支持，我们将不尽感激，或与我们联系，QQ群二维码：<br>
<img src="resources/QQgroup.jpg" width="300px">


## 致谢(Acknowledgments)
[1]ChatRWKV:https://github.com/BlinkDL/ChatRWKV<br>
[2]ChatGLM:https://github.com/THUDM/ChatGLM-6B<br>
[3]Alpaca:https://github.com/tatsu-lab/stanford_alpaca<br>
