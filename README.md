# ChatGLM-Uni
# Can lora weights be used Exchange-Fusion in different large language models?（lora权值可以在不同的大语言模型间交叉融合使用吗？）

##介绍

##方法

##实验环境

##实验结果

##后续课题


## 更新日志
2023-04-09
工程初步启动，可使用LLAMA Alpaca训练的权值，用于ChatGlm，相比LLAMA，ChatGlm以更小的模型体积和部署成本达到了近乎LLAMA的效果，尤其在英文的表现上.

## 共创共赢
如您对我们的工作产生兴趣，请给予小星星Star关注和支持，我们将不尽感激，或与我们联系，QQ群二维码：
![Contact](resources/QQgroup.png)


## 致谢
[1]ChatRWKV:https://github.com/BlinkDL/ChatRWKV<br>
[2]ChatGLM:https://github.com/THUDM/ChatGLM-6B<br>
[3]Alpaca:https://github.com/tatsu-lab/stanford_alpaca<br>
