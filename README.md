# ChatGLM-Uni
# Can lora weights be used Exchange-Fusion in different large language models?（lora权值可以在不同的大语言模型间交叉融合使用吗？）
实现一种基于Chatglm6B预训练模型+LLAMA+Alpaca Lora的融合方案，该方案简单易行，目标是使此类语言模型实现低能耗广泛部署，并最终在小模型的基座上实现“智能涌现”

# 更新日志
2023-04-09
工程初步启动，可使用LLAMA Alpaca训练的权值，用于ChatGlm，相比LLAMA，ChatGlm以更小的模型体积和部署成本达到了近乎LLAMA的效果，尤其在英文的表现上

# 共创共赢
如您对我们的工作产生兴趣，请加入我们的QQ群号：731968085

# 致谢
[1]ChatRWKV:https://github.com/BlinkDL/ChatRWKV<br>
[2]ChatGLM:https://github.com/THUDM/ChatGLM-6B<br>
[3]Alpaca:https://github.com/tatsu-lab/stanford_alpaca<br>
